## üêç AI Ouroboros Simulation: Multi-Model Collapse and Mitigation

### üí° Project Overview

This project simulates the phenomenon of **AI Model Collapse** in a more realistic, heterogeneous environment. Traditional model collapse suggests that if an AI model is iteratively trained on its own prior generations (a closed feedback loop), the quality and diversity of its output will inevitably degrade. This project extends that idea to a scenario where **multiple distinct AI models interact**, and their outputs are mixed with new **human-generated content** before being used to train the next hypothetical generation of models.

The simulation tests the hypothesis that the introduction of diversity‚Äîfrom multiple generative models and continuous human contributions‚Äîmay mitigate or alter the trajectory of model collapse.

The primary script, `simulation.py`, implements a fixed-evaluation, sliding-window perplexity, and multi-model generation strategy to quantify changes in content quality and diversity as the proportion of AI-generated content increases.

### ‚ùì Core Questions Addressed

1.  **Impact on Content Quality:** Will the quality of content generated by future AI models **dramatically decline, slowly decline, or not be impacted** by the increased use of synthetic data?
2.  **Role of Multi-Model Interaction:** How does the interaction and mixture of content from **multiple distinct AI models** (each with its own "flavor" or bias) affect the overall content pool's stability?
3.  **Mitigation:** Does the presence of different model outputs and new human content **mitigate the impact of AI model collapse**?
4.  **Long-Term Patterns:** What are the measurable **long-term patterns** that emerge in metrics like Perplexity, KL-Divergence, and Distinct-N?

### üõ†Ô∏è Simulation Methodology


1.  **Human Data Baseline:** A fixed set of human-generated texts (from the `ag_news` dataset) establishes the *target distribution* and **baseline quality**.
2.  **Multi-Model Generation:** Synthetic content is generated using **three distinct language models** specified in the script (`TinyLlama`, `phi-1_5`, `distilgpt2`).
3.  **Mixture ($\alpha$ Sweep):** For each generative model, a mixture corpus is created by blending human text and synthetic text. The **alpha ($\alpha$) parameter** (from 0.0 to 1.0) represents the **proportion of synthetic content** in the corpus.
4.  **Fixed Evaluation:** The mixed corpus is evaluated using a **fixed, high-capacity evaluation model** (`gpt2-large` or `gpt2-medium` depending on device availability) for stable metrics.
    * **Perplexity (PPL):** Measures content coherence/quality.
    * **KL-Divergence (KL):** Measures the N-gram distribution drift from the original human baseline.
    * **Distinct-2 (D2):** Measures content diversity (using bigrams). A **smoothed** version is reported to reduce noise.

### üì¶ Repository Contents

* `simulation.py`: The main Python script that executes the simulation.
* `requirements.txt`: List of necessary Python packages.
* `ai_ouroboros_multi_model_ppl_fixed_smoothed.csv`: The primary output file containing the results (Perplexity, KL-Divergence, and Smoothed Distinct-2) for each generative model and $\alpha$ value.

### üöÄ Running the Simulation

#### Prerequisites

* Python 3.8+
* A machine with a **dedicated GPU (CUDA recommended)** and sufficient VRAM (at least 12GB is highly recommended for the larger models).